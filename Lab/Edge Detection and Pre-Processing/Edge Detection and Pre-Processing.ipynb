{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "tut_1_2021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfx4O-trftDf"
      },
      "source": [
        "# CO70058 Computer Vision\n",
        "# Tutorial 1 - Edge Detection and Pre-processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O098al0jwk6-"
      },
      "source": [
        "# Convolution\n",
        "\n",
        "# Why do we need to flip the mask?\n",
        "\n",
        "Because otherwise we would be doing a cross-correlation instead of a convolution.\n",
        "\n",
        "\n",
        "\n",
        "Watch this video:\n",
        "[Convolution vs Cross Correlation](https://www.youtube.com/watch?v=C3EEy8adxvc). For a 3x3 kernel, **k**=1. **h** is the kernel, and the coordinate 0,0 of the kernel is the pixel in the centre. **F** is input image and **G** is the resulting image after the convolution.\n",
        "\n",
        "If you still have questions let me know and I will add more detail here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFuTKxzrwtdU"
      },
      "source": [
        "# Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eaaiB4rwhQL"
      },
      "source": [
        "We will be using OpenCV and Numpy so let's first import these packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeqZmHBTftDh"
      },
      "source": [
        "import cv2\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaI0BRi_ftDp"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "First we need to create the image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ELQFvMpftDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "060545da-963a-4147-cb0b-7e704dbbcc7b"
      },
      "source": [
        "img_row = np.array([3, 4, 8, 15, 25, 44, 50, 52], dtype=np.uint8)\n",
        "img_1 = np.tile(img_row, (8, 1))\n",
        "print(img_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3  4  8 15 25 44 50 52]\n",
            " [ 3  4  8 15 25 44 50 52]\n",
            " [ 3  4  8 15 25 44 50 52]\n",
            " [ 3  4  8 15 25 44 50 52]\n",
            " [ 3  4  8 15 25 44 50 52]\n",
            " [ 3  4  8 15 25 44 50 52]\n",
            " [ 3  4  8 15 25 44 50 52]\n",
            " [ 3  4  8 15 25 44 50 52]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40pzBqNyftDy"
      },
      "source": [
        "### 1.A - Prewitt operator\n",
        "\n",
        "To compute the convolution we will use the [filter2D](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga27c049795ce870216ddfb366086b5a04) function of OpenCV. If you open that link you will see them saying that that function computes correlation, not the convolution! That is, the kernel is not mirrored around the anchor point. If you need a real convolution, flip the kernel using flip and set the new anchor to `(kernel.cols - anchor.x - 1, kernel.rows - anchor.y - 1)`.\n",
        "\n",
        "In our case, given the 3x3 kernels, we can assume that the anchor of the kernel is in the middle of the image and therefore the new anchor will remain in the same position. So all we need to do is to flip the kernel twice, once relative to x and once relative to y.\n",
        "\n",
        "Let's start by creating the kernels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD-2eptnftDz"
      },
      "source": [
        "prewitt_x = np.array([[-1, 0, 1],\n",
        "                      [-1, 0, 1],\n",
        "                      [-1, 0, 1]], dtype=np.float32)\n",
        "prewitt_y = np.array([[ 1, 1, 1],\n",
        "                      [ 0, 0, 0],\n",
        "                      [-1,-1,-1]], dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldw0InF7ftD5"
      },
      "source": [
        "Now let's flip them using [np.flip()](https://numpy.org/doc/stable/reference/generated/numpy.flip.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1mMXZPaftD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5646a4-5d5d-43c5-904b-2b69cdd026ca"
      },
      "source": [
        "prewitt_x_flip = np.flip(prewitt_x, 1)\n",
        "prewitt_x_flip = np.flip(prewitt_x_flip, 0)\n",
        "print('prewitt_x_flip:\\n{}'.format(prewitt_x_flip))\n",
        "\n",
        "prewitt_y_flip = np.flip(prewitt_y, 1)\n",
        "prewitt_y_flip = np.flip(prewitt_y_flip, 0)\n",
        "print('prewitt_y_flip:\\n{}'.format(prewitt_y_flip))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prewitt_x_flip:\n",
            "[[ 1.  0. -1.]\n",
            " [ 1.  0. -1.]\n",
            " [ 1.  0. -1.]]\n",
            "prewitt_y_flip:\n",
            "[[-1. -1. -1.]\n",
            " [ 0.  0.  0.]\n",
            " [ 1.  1.  1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2L19XEiftD-"
      },
      "source": [
        "Now let's do the convolution. We add the absolute values together so that we don't cancel the effect of the different gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4-9unvIftD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c4475ba-7b05-4383-f7d9-8fc644a1f889"
      },
      "source": [
        "img_prewitt_x = cv2.filter2D(img_1, cv2.CV_64F, prewitt_x_flip)\n",
        "img_prewitt_y = cv2.filter2D(img_1, cv2.CV_64F, prewitt_y_flip)\n",
        "img_prewitt = abs(img_prewitt_x) + abs(img_prewitt_y)\n",
        "print(\"\\t1_A : \\n{}\".format(img_prewitt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t1_A : \n",
            "[[ 0. 15. 33. 51. 87. 75. 24.  0.]\n",
            " [ 0. 15. 33. 51. 87. 75. 24.  0.]\n",
            " [ 0. 15. 33. 51. 87. 75. 24.  0.]\n",
            " [ 0. 15. 33. 51. 87. 75. 24.  0.]\n",
            " [ 0. 15. 33. 51. 87. 75. 24.  0.]\n",
            " [ 0. 15. 33. 51. 87. 75. 24.  0.]\n",
            " [ 0. 15. 33. 51. 87. 75. 24.  0.]\n",
            " [ 0. 15. 33. 51. 87. 75. 24.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BQx-WUYftEE"
      },
      "source": [
        "By default OpenCV uses as a border the BORDER_REFLECT_101, [see more here](https://docs.opencv.org/master/d2/de8/group__core__array.html#ga209f2f4869e304c82d07739337eae7c5).\n",
        "\n",
        "When you are doing it by hand you can choose any type of border that you want, you could for example set all the border values to zero. So the values in the border will change according the border that you define.\n",
        "\n",
        "Technical note: use `cv2.CV_64F` if you are unsure about which type to use in the second argument of the function. Essentialy here we want to avoid [missing edges](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_gradients/py_gradients.html#one-important-matter).\n",
        "\n",
        "An alternative way to combine the gradients would be using [convertScaleAbs()](https://docs.opencv.org/3.4/d2/de8/group__core__array.html#ga3460e9c9f37b563ab9dd550c4d8c4e7d) and [addWeighted()](https://docs.opencv.org/3.4/d2/de8/group__core__array.html#gafafb2513349db3bcff51f54ee5592a19), as done in [this OpenCV tutorial](https://docs.opencv.org/3.4/d2/d2c/tutorial_sobel_derivatives.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cClFAgAckdq6"
      },
      "source": [
        "# Scales, calculates absolute values, and converts the result to 8-bit (CV_8U)\n",
        "abs_grad_x = cv2.convertScaleAbs(img_prewitt_x)\n",
        "abs_grad_y = cv2.convertScaleAbs(img_prewitt_y)\n",
        "    \n",
        "# Calculate the weighted sum of the two gradients\n",
        "grad = cv2.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0) # Some people just prefer to sum directly (abs_grad_x + abs_grad_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5yRuOsdftEF"
      },
      "source": [
        "### 1.B - Sobel\n",
        "\n",
        "Let's do the same but for the sobel now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdS7YCNfftEG"
      },
      "source": [
        "sobel_x = np.array([[-1, 0, 1],\n",
        "                    [-2, 0, 2],\n",
        "                    [-1, 0, 1]], dtype=np.float32)\n",
        "sobel_y = np.array([[ 1, 2, 1],\n",
        "                    [ 0, 0, 0],\n",
        "                    [-1,-2,-1]], dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mna391pftEJ"
      },
      "source": [
        "Let's flip:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw-cGm63ftEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdca5f53-82fb-4085-812d-3f85ce50f38b"
      },
      "source": [
        "sobel_x_flip = np.flip(sobel_x, 1)\n",
        "sobel_x_flip = np.flip(sobel_x_flip, 0)\n",
        "print('sobel_x_flip:\\n{}'.format(sobel_x_flip))\n",
        "\n",
        "sobel_y_flip = np.flip(sobel_y, 1)\n",
        "sobel_y_flip = np.flip(sobel_y_flip, 0)\n",
        "print('sobel_y_flip:\\n{}'.format(sobel_y_flip))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sobel_x_flip:\n",
            "[[ 1.  0. -1.]\n",
            " [ 2.  0. -2.]\n",
            " [ 1.  0. -1.]]\n",
            "sobel_y_flip:\n",
            "[[-1. -2. -1.]\n",
            " [ 0.  0.  0.]\n",
            " [ 1.  2.  1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5s3Iy_IftEO"
      },
      "source": [
        "And the convolution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4Fcci-2ftEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9abf6fd8-e241-4a6c-f8f8-a4eb530eff68"
      },
      "source": [
        "img_sobel_x = cv2.filter2D(img_1, cv2.CV_64F, sobel_x_flip)\n",
        "img_sobel_y = cv2.filter2D(img_1, cv2.CV_64F, sobel_y_flip)\n",
        "img_sobel = abs(img_sobel_x) + abs(img_sobel_y)\n",
        "print(\"\\t1_B : \\n{}\".format(img_sobel))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t1_B : \n",
            "[[  0.  20.  44.  68. 116. 100.  32.   0.]\n",
            " [  0.  20.  44.  68. 116. 100.  32.   0.]\n",
            " [  0.  20.  44.  68. 116. 100.  32.   0.]\n",
            " [  0.  20.  44.  68. 116. 100.  32.   0.]\n",
            " [  0.  20.  44.  68. 116. 100.  32.   0.]\n",
            " [  0.  20.  44.  68. 116. 100.  32.   0.]\n",
            " [  0.  20.  44.  68. 116. 100.  32.   0.]\n",
            " [  0.  20.  44.  68. 116. 100.  32.   0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGwMT50WftET"
      },
      "source": [
        "Now, you may be asking: \"Couldn't we have used the OpenCV inbuilt function of sobel [cv2.Sobel()](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d)?\"\n",
        "\n",
        "Well, let's try:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pszlVjFvftEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f532756-24b9-45d9-8a46-88d4b326c1a5"
      },
      "source": [
        "img_sobel_x = cv2.Sobel(img_1, cv2.CV_64F, 1, 0, ksize=3)\n",
        "img_sobel_y = cv2.Sobel(img_1, cv2.CV_64F, 0, 1, ksize=3)\n",
        "img_sobel = abs(img_sobel_x) + abs(img_sobel_y)\n",
        "print(\"\\t1_B : \\n{}\".format(img_sobel))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t1_B : \n",
            "[[  0.  20.  44.  68. 116. 100.  32.   0.]\n",
            " [  0.  20.  44.  68. 116. 100.  32.   0.]\n",
            " [  0.  20.  44.  68. 116. 100.  32.   0.]\n",
            " [  0.  20.  44.  68. 116. 100.  32.   0.]\n",
            " [  0.  20.  44.  68. 116. 100.  32.   0.]\n",
            " [  0.  20.  44.  68. 116. 100.  32.   0.]\n",
            " [  0.  20.  44.  68. 116. 100.  32.   0.]\n",
            " [  0.  20.  44.  68. 116. 100.  32.   0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDV-XsKKftEX"
      },
      "source": [
        "We get the same!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IYkZ0L6ftEZ"
      },
      "source": [
        "### 1.C - Laplacian\n",
        "\n",
        "Let's do the same:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o2e2jwDftEZ"
      },
      "source": [
        "laplacian = np.array([[0,  1, 0],\n",
        "                      [1, -4, 1],\n",
        "                      [0,  1, 0]], dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7xHj5nhftEd"
      },
      "source": [
        "In this case the kernel is symmetrical so we do not need to worry about flipping!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJQqekFKftEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83bbdfb8-dbe9-4d17-fc5e-b041844de3f8"
      },
      "source": [
        "img_laplacian = cv2.filter2D(img_1, cv2.CV_64F, laplacian)\n",
        "print(\"\\t1_C : \\n{}\".format(img_laplacian))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t1_C : \n",
            "[[  2.   3.   3.   3.   9. -13.  -4.  -4.]\n",
            " [  2.   3.   3.   3.   9. -13.  -4.  -4.]\n",
            " [  2.   3.   3.   3.   9. -13.  -4.  -4.]\n",
            " [  2.   3.   3.   3.   9. -13.  -4.  -4.]\n",
            " [  2.   3.   3.   3.   9. -13.  -4.  -4.]\n",
            " [  2.   3.   3.   3.   9. -13.  -4.  -4.]\n",
            " [  2.   3.   3.   3.   9. -13.  -4.  -4.]\n",
            " [  2.   3.   3.   3.   9. -13.  -4.  -4.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoFdz0olftEi"
      },
      "source": [
        "Now, let's do it with the inbuilt function of OpenCV [cv2.Laplacian()](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gad78703e4c8fe703d479c1860d76429e6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23sPCIMdftEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ef87e4-c49e-4aa4-95cd-61319695faef"
      },
      "source": [
        "img_laplacian = cv2.Laplacian(img_1, cv2.CV_64F, ksize=1) # ksize=1 gives the expected kernel: https://docs.opencv.org/3.4/da/d85/tutorial_js_gradients.html\n",
        "print(\"\\t1_C : \\n{}\".format(img_laplacian))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t1_C : \n",
            "[[  2.   3.   3.   3.   9. -13.  -4.  -4.]\n",
            " [  2.   3.   3.   3.   9. -13.  -4.  -4.]\n",
            " [  2.   3.   3.   3.   9. -13.  -4.  -4.]\n",
            " [  2.   3.   3.   3.   9. -13.  -4.  -4.]\n",
            " [  2.   3.   3.   3.   9. -13.  -4.  -4.]\n",
            " [  2.   3.   3.   3.   9. -13.  -4.  -4.]\n",
            " [  2.   3.   3.   3.   9. -13.  -4.  -4.]\n",
            " [  2.   3.   3.   3.   9. -13.  -4.  -4.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6EmpZH_ftEn"
      },
      "source": [
        "We get the same!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXgB15YIftEo"
      },
      "source": [
        "Comment on 1:\n",
        "\n",
        "From the Prewitt and Sobel Operator we can see that in the edge area, the pixel intensity shows a \"jump\" or a high variation of intensity. Getting the first derivative of the intensity, we observe that an edge is characterized by a maximum or a minimum. What happens if we take the second derivative? On the edge, the second derivative is zero! So, we can also use this criterion to attempt to detect edges in an image. However, note that zeros will not only appear in edges (they can actually appear in other meaningless locations); this can be solved by applying filtering where needed.\n",
        "    \n",
        "[Read more](https://docs.opencv.org/3.4/d5/db5/tutorial_laplace_operator.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmz3ASONftEo"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "First we need to create the image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWTrWT2nftEp"
      },
      "source": [
        "img_2 = np.array([[7, 12, 9],\n",
        "                  [6,  7, 8],\n",
        "                  [3,  4, 5]], dtype=np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAEuxGrwftEs"
      },
      "source": [
        "Let's use the flipped kernels again to do the convolution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdPHJ_UnftEs"
      },
      "source": [
        "grad_x_a = cv2.filter2D(img_2, cv2.CV_64F, prewitt_x_flip)\n",
        "grad_y_a = cv2.filter2D(img_2, cv2.CV_64F, prewitt_y_flip)\n",
        "grad_x_a = abs(grad_x_a)\n",
        "grad_y_a = abs(grad_y_a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMBr70VnftEv"
      },
      "source": [
        "Now let's compute the magnitude and direction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch2SSS9XftEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f85ebcad-764e-4b50-ef6d-b0f252a7aee3"
      },
      "source": [
        "# Get square root of sum of squares\n",
        "magnitude_a = np.hypot(grad_x_a, grad_y_a)\n",
        "print('magnitude:\\n{}'.format(magnitude_a))\n",
        "# Get angles in radians\n",
        "grad_direction_a = np.arctan2(grad_y_a, grad_x_a)\n",
        "# Convert radians to degrees\n",
        "grad_direction_a = grad_direction_a * 180 / np.pi\n",
        "print('grad_direction:\\n{}'.format(grad_direction_a))\n",
        "print(\"\\t2_A:\\n{} degrees\".format(grad_direction_a[1, 1])) # Get pixel in the middle [1, 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "magnitude:\n",
            "[[ 0.          6.          0.        ]\n",
            " [20.         17.08800749 20.        ]\n",
            " [ 0.          6.          0.        ]]\n",
            "grad_direction:\n",
            "[[ 0.          0.          0.        ]\n",
            " [90.         69.44395478 90.        ]\n",
            " [ 0.          0.          0.        ]]\n",
            "\t2_A:\n",
            "69.44395478041653 degrees\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCYsy0WiftE0"
      },
      "source": [
        "Now let's do the same for the Sobel kernel:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOpKZin6ftE0"
      },
      "source": [
        "grad_x_b = cv2.filter2D(img_2, cv2.CV_64F, sobel_x_flip)\n",
        "grad_y_b = cv2.filter2D(img_2, cv2.CV_64F, sobel_y_flip)\n",
        "grad_x_b = abs(grad_x_b)\n",
        "grad_y_b = abs(grad_y_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSbOtrsLftE3"
      },
      "source": [
        "Now let's compute the magnitude and direction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cOWVdKdftE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f9a03d-3088-4b23-9b51-cd58538b8dd2"
      },
      "source": [
        "# Get square root of sum of squares\n",
        "magnitude_b = np.hypot(grad_x_b, grad_y_b)\n",
        "print('magnitude:\\n{}'.format(magnitude_b))\n",
        "# Get angles in radians\n",
        "grad_direction_b = np.arctan2(grad_y_b, grad_x_b)\n",
        "# Convert radians to degrees\n",
        "grad_direction_b = grad_direction_b * 180 / np.pi\n",
        "print('grad_direction:\\n{}'.format(grad_direction_b))\n",
        "print(\"\\t2_B:\\n{} degrees\".format(grad_direction_b[1, 1])) # Get pixel in the middle [1, 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "magnitude:\n",
            "[[ 0.          8.          0.        ]\n",
            " [24.         25.29822128 24.        ]\n",
            " [ 0.          8.          0.        ]]\n",
            "grad_direction:\n",
            "[[ 0.          0.          0.        ]\n",
            " [90.         71.56505118 90.        ]\n",
            " [ 0.          0.          0.        ]]\n",
            "\t2_B:\n",
            "71.56505117707799 degrees\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtYf9MI_ftE9"
      },
      "source": [
        "Comment on 2:\n",
        "\n",
        "    Both the results are very similar. Prewitt operator is similar to the Sobel operator and is used for detecting vertical and horizontal edges in images. However, unlike the Sobel, this operator does not place any emphasis on the pixels that are closer to the center of the mask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RlK5J7wftE-"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Let's start by creating the kernel, notice that it is simmetric so no need to flip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1uObX5XftE-"
      },
      "source": [
        "gaussian = np.array([[1.0/36, 1.0/9, 1.0/36],\n",
        "                     [1.0/9 , 4.0/9, 1.0/9 ],\n",
        "                     [1.0/36, 1.0/9, 1.0/36]], dtype=np.float64) # float64 since we will be doing a SVD decomposition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcEH7CI0ftFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf7c981-7398-4cfd-98b7-edc2aac6117e"
      },
      "source": [
        "img_gaussian = cv2.filter2D(img_1, cv2.CV_64F, gaussian)\n",
        "print(\"\\tQ4: Filtered image:\\n{}\".format(img_gaussian))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tQ4: Filtered image:\n",
            "[[ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]\n",
            " [ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]\n",
            " [ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]\n",
            " [ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]\n",
            " [ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]\n",
            " [ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]\n",
            " [ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]\n",
            " [ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukk_LETlftFM"
      },
      "source": [
        "Another option would be to use the OpenCV inbuilt function [cv2.GaussianBlur()](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1) to compute the Gaussian:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH9dHuegftFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfb0f4f-90f3-49ab-879a-348e6b38ef41"
      },
      "source": [
        "img_gaussian = cv2.GaussianBlur(img_1, (3, 3), 0)\n",
        "print(img_gaussian)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 4  5  9 16 27 41 49 51]\n",
            " [ 4  5  9 16 27 41 49 51]\n",
            " [ 4  5  9 16 27 41 49 51]\n",
            " [ 4  5  9 16 27 41 49 51]\n",
            " [ 4  5  9 16 27 41 49 51]\n",
            " [ 4  5  9 16 27 41 49 51]\n",
            " [ 4  5  9 16 27 41 49 51]\n",
            " [ 4  5  9 16 27 41 49 51]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "546dFh9QftFP"
      },
      "source": [
        "Filtering an M-by-N image with a P-by-Q filter kernel requires roughly MNPQ multiplies and MN(PQ - 1) adds.\n",
        "In our case with a 3x3 kernel, P=Q=3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIf5Da5BftFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f31c31-d1c9-4042-9a49-260a204069a0"
      },
      "source": [
        "img_h, img_w = img_1.shape\n",
        "P = 3.0\n",
        "Q = 3.0\n",
        "n_multiplications = img_h * img_w * P * Q\n",
        "n_additions = img_h * img_w * ((P * Q) - 1.0)\n",
        "print(\"Computation cost:\\n {} multiplications and {} additions\".format(int(n_multiplications), int(n_additions)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computation cost:\n",
            " 576 multiplications and 512 additions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxN5fXztftFU"
      },
      "source": [
        "Now how could we separate the 2D kernel into two 1D kernels?\n",
        "\n",
        "Well, we can do that if it is a separable filter.\n",
        "[What is a separable filter?](https://blogs.mathworks.com/steve/2006/10/04/separable-convolution/)\n",
        "\n",
        "Basically, it is a two-dimensional filter kernel is separable if it can be expressed as the `outer product` of two vectors.\n",
        "\n",
        "[How can I determine if a matrix is an outer product of two vectors? How to determine these vectors?](https://blogs.mathworks.com/steve/2006/11/28/separable-convolution-part-2/)\n",
        "\n",
        "Well if it is separable we will find the two vectors using an SVD decomposition as mentioned in that website.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-GmH9dnftFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9365d0fb-74a5-4171-867b-fb45351a344c"
      },
      "source": [
        "u, s, vh  = np.linalg.svd(gaussian)\n",
        "\n",
        "d_y = u[:,0] * np.sqrt(s[0]) # should give this result = [1/6, 2/3, 1/6]\n",
        "print(\"\\tQ4: Calculated d_y:\\n{}\".format(d_y))\n",
        "\n",
        "d_x = np.transpose(vh)[:,0] * np.sqrt(s[0]) # should give this result = [1/6, 2/3, 1/6]\n",
        "print(\"\\tQ4: Calculated d_x:\\n{}\".format(d_x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tQ4: Calculated d_y:\n",
            "[-0.16666667 -0.66666667 -0.16666667]\n",
            "\tQ4: Calculated d_x:\n",
            "[-0.16666667 -0.66666667 -0.16666667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IQvJ0oyftFX"
      },
      "source": [
        "If these are indeed the 1D kernels then their outer product should give the same as the gaussian kernel.\n",
        "Let's try that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNfrqi1XftFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af62caf6-ff7e-484d-82cb-1cbd9c6404f3"
      },
      "source": [
        "print(\"Should be zero:\\n{}\".format(gaussian - np.outer(d_y, d_x)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Should be zero:\n",
            "[[ 1.38777878e-17  9.71445147e-17  2.08166817e-17]\n",
            " [-2.77555756e-17  0.00000000e+00 -1.38777878e-17]\n",
            " [-6.93889390e-18  0.00000000e+00 -3.46944695e-18]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmQojyEO4WmO"
      },
      "source": [
        "Which is aprox. 0 as expected!\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJrirbwtftFb"
      },
      "source": [
        "\n",
        "\n",
        "Let's give it a try with the 1D convolutions\n",
        "   - Here we can use [np.convolve](https://numpy.org/doc/stable/reference/generated/numpy.convolve.html) to do 1D convolutions\n",
        "\n",
        "To get the same result as OpenCV first we need to add the same border around it\n",
        "   - since we have a 3x3 kernel we need to add a border of size two.\n",
        "   - also by default OpenCV used BORDER_REFLECT_101 when doing convolutions, also let's do the same here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5iB4LysftFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f5f3c4-4b7e-4c7d-f7d1-15a4eeedeb6b"
      },
      "source": [
        "BORDER_SIDE = 1\n",
        "img_4 = cv2.copyMakeBorder(img_1, BORDER_SIDE, BORDER_SIDE, BORDER_SIDE, BORDER_SIDE, borderType=cv2.BORDER_REFLECT_101)\n",
        "print(\"Image with border:\\n{}\".format(img_4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image with border:\n",
            "[[ 4  3  4  8 15 25 44 50 52 50]\n",
            " [ 4  3  4  8 15 25 44 50 52 50]\n",
            " [ 4  3  4  8 15 25 44 50 52 50]\n",
            " [ 4  3  4  8 15 25 44 50 52 50]\n",
            " [ 4  3  4  8 15 25 44 50 52 50]\n",
            " [ 4  3  4  8 15 25 44 50 52 50]\n",
            " [ 4  3  4  8 15 25 44 50 52 50]\n",
            " [ 4  3  4  8 15 25 44 50 52 50]\n",
            " [ 4  3  4  8 15 25 44 50 52 50]\n",
            " [ 4  3  4  8 15 25 44 50 52 50]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vextEovVftFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5b1e89-dc82-4c69-d2c3-3097d537b230"
      },
      "source": [
        "img_h_with_broder, img_w_with_broder = img_4.shape\n",
        "result = np.zeros((img_h_with_broder, img_w_with_broder))\n",
        "\n",
        "for v in range(img_h_with_broder):\n",
        "    result[v] = np.convolve(img_4[v,:], d_x, 'same')\n",
        "for u in range(img_w_with_broder):\n",
        "    result[:,u] = np.convolve(result[:,u], d_y, 'same')\n",
        "\n",
        "# Finally let's crop again to get the initial size\n",
        "result = result[BORDER_SIDE:BORDER_SIDE+img_h, BORDER_SIDE:BORDER_SIDE+img_w]\n",
        "print(\"\\tQ4: Filtered image using 2x 1D vectors:\\n{}\".format(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tQ4: Filtered image using 2x 1D vectors:\n",
            "[[ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]\n",
            " [ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]\n",
            " [ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]\n",
            " [ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]\n",
            " [ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]\n",
            " [ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]\n",
            " [ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]\n",
            " [ 3.33333333  4.5         8.5        15.5        26.5        41.83333333\n",
            "  49.33333333 51.33333333]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oErMn8ydftFg"
      },
      "source": [
        "We got the same result as before! Now let's look into the computational cost:\n",
        "\n",
        "Theoretically, when filtering an M-by-N image with a P-by-Q filter kernel, if the kernel is separable, you can filter in two steps.\n",
        "The first step requires about MNP multiplies and MN(P-1) adds. The second requires about MNQ multiplies and MN(Q-1) adds, for a total of MN(P + Q) multiplies and MN(P + Q - 2) adds.\n",
        "\n",
        "In our case with a 3x3 kernel, P=Q=3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy4myDWbftFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a80692bb-3991-4f63-8dd7-a323af7d6f54"
      },
      "source": [
        "n_multiplications = img_h * img_w * (P + Q)\n",
        "n_additions = img_h * img_w * (P + Q - 2.0)\n",
        "print(\"Computation cost:\\n {} multiplications and {} additions\".format(int(n_multiplications), int(n_additions)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computation cost:\n",
            " 384 multiplications and 256 additions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bf0odRoftFj"
      },
      "source": [
        "As we see it's better than before. However, in practice we should take into consideration that OpenCV is highly optimized and for sufficiently large kernels (~11 x 11 or larger) it uses the discrete Fourier Transform to speed up the process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJg1ypN5ftE-"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "You will learn in the next tutorials more about 3D reconstruction. For 3D reconstruction most algorithms match regions (such as corners and edges) between multiple pictures of the same scene. However, in the presence of reflections, the algorithm may get confused since it could match the features in the reflection with features in the physical objects.\n",
        "\n",
        "With the coming tutorials I think you will understand what I am talking about here, but if you don't please just ask!"
      ]
    }
  ]
}